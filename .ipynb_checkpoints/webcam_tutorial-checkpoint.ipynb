{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93829b81",
   "metadata": {},
   "source": [
    "2022-04-15 이주형\n",
    "\n",
    "# 실시간 감정인식 데모 실행 코드 튜토리얼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca183f91",
   "metadata": {},
   "source": [
    "라이브러리 import 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42117f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import torchvision.models as models\n",
    "# import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import model.efficientNetV2 as models\n",
    "from PIL import Image\n",
    "import torchvision.datasets as datasets\n",
    "from utils.transforms import transforms_test as transform\n",
    "from utils.transforms import NormalizePerImage\n",
    "import glob\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "import shutil\n",
    "from utils.facenet_pytorch import MTCNN\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29108d8f",
   "metadata": {},
   "source": [
    "## argument 파싱 부분\n",
    "\n",
    "args.model: 모델웨이트 path 부분\\\n",
    "args.gpu: gpu 번호 명시\\\n",
    "args.resize_h: 0보다 크면 카메라에서 실시간으로 받아진 영상을 args.resize_h로 resize 합니다. default: 0 (resize 안함):\n",
    "args.fps: frame per second 입니다.\n",
    "***args.entropy_threshold***: 신뢰도와 관련된 숫자이며 ***entropy가 작을수록 신뢰도가 높은 것입니다 (엄격한 기준)***. 디폴트값은 2 이며 상황보며 경험적으로 조절하면 됩니다. 신뢰도가 적으면 추론된 감정을 표기해주지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356db92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Emotion inference from cropped face image')\n",
    "parser.add_argument('--model', type=str, default='checkpoints/efficientNetV2_m.pth.tar')\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "parser.add_argument('--resize-h', type=float, default=0)\n",
    "parser.add_argument('--fps', type=int, default=50)\n",
    "parser.add_argument('--entropy-threshold', type=float, default=2)\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69027263",
   "metadata": {},
   "source": [
    "EMOTION: 감정을 텍스트로 실시간으로 띄워주는 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMOTION = ['기쁨', '당황', '분노', '불안', '상처', '슬픔', '중립']\n",
    "EMOTION =['HAPPY', 'EMBARRASED', 'ANGRY', 'ANXIOUS', 'HURT', 'SAD', 'NEUTRAL']\n",
    "FONT_SCALE=0.5\n",
    "FONT_COLOR=(255,255,255)\n",
    "FONT_THICKNESS=2\n",
    "normalize=NormalizePerImage()\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e59c48",
   "metadata": {},
   "source": [
    "ipcamera 주소입니다.\\\n",
    "ipcamera를 사용하지 않는 경우에 바로 아래의 라인을 사용하시면 됩니다. ```camera = cv2.VideoCapture(0)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f960c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture('rtsp://192.168.10.101:554/media/1/1')\n",
    "# camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94c1324",
   "metadata": {},
   "source": [
    "얼굴인식모델과 감정인식 모델을 가져와서 사용할 디바이스에 카피시키고\\\n",
    "감정인식 모델의 웨이트 파일을 가져와서 감정인식 모델에 로딩시키는 부분입니다\\\n",
    "\n",
    "GPU가 아닌 CPU를 사용할 경우, 주석 처리된 두 라인으로 각라인의 바로 위 라인을 대체시키면 됩니다.\\\n",
    "\n",
    "훈련이 아니기 때문에 gradient계산및 필요없는 계산을 하지 않기 위해 eval모드를 켜줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f1399",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(image_size=160, margin=8, device=torch.device('cuda:0'))\n",
    "# mtcnn = MTCNN(image_size=160, margin=8)\n",
    "\n",
    "efficient_net = models.__dict__['effnetv2_m'](num_classes=7)\n",
    "efficient_net = efficient_net.cuda(0)\n",
    "checkpoint = torch.load(args.model, map_location=f'cuda:{args.gpu}')\n",
    "# checkpoint = torch.load(args.model)\n",
    "efficient_net.load_state_dict(checkpoint['state_dict'])\n",
    "efficient_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae74b0",
   "metadata": {},
   "source": [
    "여러 잡다하는 부분들을 해주는 부분입니다.\\\n",
    "카메라의 FPS가 ```args.fps```보다 작을 경우 코드가 돌지 않게 하는 부분은 주석 처리 해놓았습니다. ```# assert(args.fps < FPS)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6472c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow('DEMO')\n",
    "\n",
    "(major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
    "\n",
    "if (int(major_ver) < 3):\n",
    "    FPS = camera.get(cv2.CV_CAP_PROP_FPS)\n",
    "else:\n",
    "    FPS = camera.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# assert(args.fps < FPS)\n",
    "# SKIP_FRAME = round(FPS/float(args.fps))\n",
    "\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c0020",
   "metadata": {},
   "source": [
    "실시간 캠을 통한 감정인식이기 때문에 ```while True```를 수행합니다.\\\n",
    "프레임이 제대로 들어왔다면 ```check```가 ```True```값이 됩니다.\\\n",
    "실시간 캠에서 프레임영상을 ```frame```에 저장하고 ```args.resize_h > 0```의 경우, ```frame```을 ```args.resize_h```에 맞게 resize 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf05b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    i += 1\n",
    "    # if i == SKIP_FRAME:\n",
    "    check, frame = camera.read()\n",
    "    print(f'frame: {frame.shape}')\n",
    "    if args.resize_h > 0:\n",
    "        frame = cv2.resize(frame, (int(args.resize_h), int(args.resize_h*frame.shape[0]/float(frame.shape[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce14ab9b",
   "metadata": {},
   "source": [
    "프레임이 제대로 들어온 경우, MTCNN을 통해 얼굴부분을 자른 영상과 (```faces```) bounding box coordinate를 (```boxes```) 가져옵니다.\\\n",
    "\n",
    "face가 존재할 경우(```if faces != None```), 그라디언트 계산을 꺼주고 (```with torch.no_grad()```), FP-16 베이스로(```with torch.cuda.amp.autocast()```) face를 normalize한 뒤 gpu로 카피해주고 (```face = normalize(faces).cuda(args.gpu, non_blocking=True)```), 모델에 feed-forward를 수행합니다 (```emotion = efficient_net(torch.unsqueeze(face, 0))```).\\\n",
    "feed-forward에서 나온 logit을 바탕으로 클래스별 확률과(```prob = torch.nn.functional.softmax(emotion)```) 엔트로피(```entropy = -torch.sum(prob * torch.log(prob))```)를 계산합니다. 엔트로피는 신뢰도로 사용할 예정입니다.\n",
    "\n",
    "opencv를 통해 화면에(```frame```) 박스를 덧입히고 (```cv2.rectangle()```), 인식된 얼굴을 통해 추론된 감정의 신뢰도가 (```entropy```) 기준점보다 높다면 (```< args.entropy_threshold```) 화면에 (```frame```) 높은 신뢰도로 추론된 감정을 텍스트로 덧입힙니다 (```cv2.putText```). 또한 신뢰도가 높을 경우, console창에도 감정을 띄워줍니다 (```print()```지워도 되요)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if check:\n",
    "\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        faces, boxes = mtcnn(frame)\n",
    "        if faces != None:\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    face = normalize(faces).cuda(args.gpu, non_blocking=True)\n",
    "                    # face = normalize(faces)\n",
    "                    emotion = efficient_net(torch.unsqueeze(face, 0))\n",
    "                    prob = torch.nn.functional.softmax(emotion)\n",
    "                    entropy = -torch.sum(prob * torch.log(prob))\n",
    "                    # print(f'entropy: {entropy}')\n",
    "                cv2.rectangle(frame, (int(boxes[0][0]), int(boxes[0][1])), (int(boxes[0][2]), int(boxes[0][3])), (255, 0, 0), 2)\n",
    "                if entropy < args.entropy_threshold:\n",
    "                    cv2.putText(frame, EMOTION[torch.argmax(emotion)], (int(boxes[0][0]), int(boxes[0][1])-10), 0, FONT_SCALE, FONT_COLOR, FONT_THICKNESS)\n",
    "\n",
    "                # draw.rectangle(boxes[0].tolist(), outline=(255, 0, 0), width=6)\n",
    "                # draw.text((int(boxes[0][0]), int(boxes[0][3])+3), EMOTION[torch.argmax(emotion)], font=ImageFont.truetype(\"fonts/gulim.ttc\", 20), align =\"left\")\n",
    "                    print(f'emotion: {EMOTION[torch.argmax(emotion)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd1908",
   "metadata": {},
   "source": [
    "얼굴위치박스와 감정 텍스트가 덧입혀진 ```frame```을 화면에 띄워줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac071ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('DEMO', frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4839331c",
   "metadata": {},
   "source": [
    "args.fps기준으로 기다립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.waitKey(int(1000/float(args.fps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca46688",
   "metadata": {},
   "source": [
    "q 를 누르면 꺼집니다. 안꺼지면 ctrl+c 누르면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6855c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30aaed",
   "metadata": {},
   "source": [
    "deestructor라고 생각하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
